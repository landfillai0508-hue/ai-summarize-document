{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e2bcc2-0c95-4523-8cbc-2d7fa3f7deee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Teenagers in foster care in Scotland are being moved too often, according to '\n",
      " 'a campaign group. Research carried out\\n'\n",
      " 'by the Fostering Network suggests almost half of fostered young people are '\n",
      " 'already living with their third foster\\n'\n",
      " 'family since going into care. The group has warned that 750 more foster '\n",
      " 'carers are \\\\\"urgently\\\\\" needed to meet the\\n'\n",
      " 'demands of the care system. It urged people to \\\\\"open their hearts and '\n",
      " 'homes\\\\\" to vulnerable youngsters. Currently,\\n'\n",
      " 'more than 5,500 children are in foster care in Scotland, living with 4,400 '\n",
      " 'families and carers. The Fostering Network\\n'\n",
      " 'surveyed 250 children, teenagers and foster carers across Scotland and '\n",
      " 'discovered that many young people had failed\\n'\n",
      " 'to find stability. Almost half were already living with their third family, '\n",
      " 'a quarter were with their fourth family\\n'\n",
      " 'and about 20 were living with their 10th family since going into care. There '\n",
      " 'was a particular need for homes to be\\n'\n",
      " 'found for vulnerable teenagers, siblings and disabled children, the\\n'\n",
      " 'study found. Carla, 23, was taken into care at the age of 12 and had eight '\n",
      " 'foster homes before moving in with the\\n'\n",
      " 'Randalls. Looking back now I realised that the Randalls saved my life,\\\\\" '\n",
      " 'she said. \\\\\"I never understood the extent\\n'\n",
      " \"of the neglect and abuse I had endured until I came to live with a 'normal' \"\n",
      " 'loving family. They were just always\\n'\n",
      " 'themselves, the smallest details meant so much to me. They nurtured a young, '\n",
      " 'angry, untrusting teenager to become a\\n'\n",
      " 'positive, empathetic and successful young woman. The Fostering Network said '\n",
      " 'instability had a detrimental effect\\n'\n",
      " \"on the child's education and wellbeing, while finding a stable foster carer \"\n",
      " 'from the outset could lead to\\n'\n",
      " 'improved relationships and a happier childhood. Sara Lurie, director of the '\n",
      " 'Fostering Network Scotland, said:\\n'\n",
      " '\\\\\"As each year passes, we see more and more children coming into care. We '\n",
      " 'need people who can open their heart, and\\n'\n",
      " 'their homes, to vulnerable children and young people and use their skills to '\n",
      " 'help support them to reach their full\\n'\n",
      " 'potential. In particular we need people who have the skills, patience and '\n",
      " 'passion to look after teenagers who may\\n'\n",
      " 'have had a really tough time and be facing some real challenges, and to '\n",
      " 'offer them love, stability and security.\\n'\n",
      " 'A good foster carer will believe in the ambition of the children in their '\n",
      " \"care in the same way they'd believe in the\\n\"\n",
      " 'ambition of their biological family members. Apologies for the delay, see '\n",
      " 'below as requested. A Scottish government\\n'\n",
      " 'spokeswoman said: \\\\\"Giving young people security is paramount and we have '\n",
      " 'done a great deal of work with our partners\\n'\n",
      " 'across local government and the third sector to improve how we intervene '\n",
      " 'early when there is a problem within families\\n'\n",
      " 'to find appropriate solutions quickly. We have also expanded the age at '\n",
      " 'which young people can remain in foster care\\n'\n",
      " 'as part of the continuing care provisions and the support available when '\n",
      " 'they transition into independent living.')\n"
     ]
    }
   ],
   "source": [
    "# load an article\n",
    "import os\n",
    "ABSOLUTE_PATH = os.getcwd()\n",
    "\n",
    "import sys\n",
    "ROOT_PATH = '/'.join(ABSOLUTE_PATH.split('/')[:-1])\n",
    "sys.path.append(ROOT_PATH)\n",
    "\n",
    "\n",
    "from main.document import Document\n",
    "from pprint import pprint\n",
    "input_file_path = f'{ROOT_PATH}/data/article.txt'\n",
    "document = Document.load_from_local(input_file_path)\n",
    "pprint(document.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf8bbe5-6904-46ad-a020-1c5f6563a7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yihong/Documents/ai-summarize-document/venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying model deepseek-r1:8b\n",
      "Summarize at iteration 0\n",
      "Summarize at iteration 1\n",
      "Summarize at iteration 2\n",
      "Number of valid reports: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid report 1; Bert-Score-F1: 0.0; Roger-Score-F1: 0.0\n",
      "Valid report 2; Bert-Score-F1: 0.6149623990058899; Roger-Score-F1: 0.36551724137931035\n",
      "Valid report 3; Bert-Score-F1: 0.6435026526451111; Roger-Score-F1: 0.3652694610778443\n",
      "Trying model qwen3-vl:8b\n",
      "Summarize at iteration 0\n",
      "Summarize at iteration 1\n",
      "Summarize at iteration 2\n",
      "Number of valid reports: 3\n",
      "Valid report 1; Bert-Score-F1: 0.6124502420425415; Roger-Score-F1: 0.421875\n",
      "Valid report 2; Bert-Score-F1: 0.6285790801048279; Roger-Score-F1: 0.40298507462686567\n",
      "Valid report 3; Bert-Score-F1: 0.6187511682510376; Roger-Score-F1: 0.41\n",
      "Trying model llama2:latest\n",
      "Summarize at iteration 0\n",
      "Summarize at iteration 1\n",
      "Summarize at iteration 2\n",
      "Number of valid reports: 3\n",
      "Valid report 1; Bert-Score-F1: 0.6888040900230408; Roger-Score-F1: 0.6217391304347826\n",
      "Valid report 2; Bert-Score-F1: 0.7325177192687988; Roger-Score-F1: 0.7955390334572491\n",
      "Valid report 3; Bert-Score-F1: 0.6820167303085327; Roger-Score-F1: 0.6376811594202898\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Common configuration\n",
    "has_title = True                   # has a title\n",
    "min_num_of_char_in_title = 32      # 32 <= title length <= 80 \n",
    "max_num_of_char_in_title = 80\n",
    "compression_rate = 0.3             # compression rate = 0.3\n",
    "min_num_of_paragraph = 2           # 2 <= number of paragraphs <= 4\n",
    "max_num_of_paragraph = 4\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "llm_api_client = AsyncOpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"dummy_key\")\n",
    "\n",
    "\n",
    "from main.summarizer import BestHitLLMSummarizer\n",
    "candidate_models = [\"deepseek-r1:8b\",\n",
    "                   \"qwen3-vl:8b\",\n",
    "                   \"llama2:latest\", ]\n",
    "per_model_report = {}\n",
    "for model in candidate_models:\n",
    "    print(f'Trying model {model}')\n",
    "    summarizer = BestHitLLMSummarizer(\n",
    "        client=llm_api_client, \n",
    "        model=model,\n",
    "        has_title=has_title,\n",
    "        min_num_of_char_in_title=min_num_of_char_in_title,\n",
    "        max_num_of_char_in_title=max_num_of_char_in_title,\n",
    "        min_num_of_paragraph=min_num_of_paragraph,\n",
    "        max_num_of_paragraph=max_num_of_paragraph,\n",
    "        num_tries=3,\n",
    "        llm_as_judge=False)\n",
    "    \n",
    "    report = await summarizer.summarize(document=document)\n",
    "    per_model_report[model] = report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "499227c1-c252-4a66-a07b-69e310764c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model deepseek-r1:8b\n",
      "Run HasTitleMetricExtractor\n",
      "Run TitleLengthMetricExtractor\n",
      "Run NumberOfParagraphMetricExtractor\n",
      "Run NumberOfTokenMetricExtractor\n",
      "Run BertScoreMetricExtractor\n",
      "Run RougeScoreMetricExtractor\n",
      "Run CorrectnessMetricExtractor\n",
      "Run CompletenessMetricExtractor\n",
      "--------------------------------------------------------------\n",
      "evaluating model qwen3-vl:8b\n",
      "Run HasTitleMetricExtractor\n",
      "Run TitleLengthMetricExtractor\n",
      "Run NumberOfParagraphMetricExtractor\n",
      "Run NumberOfTokenMetricExtractor\n",
      "Run BertScoreMetricExtractor\n",
      "Run RougeScoreMetricExtractor\n",
      "Run CorrectnessMetricExtractor\n",
      "Run CompletenessMetricExtractor\n",
      "--------------------------------------------------------------\n",
      "evaluating model llama2:latest\n",
      "Run HasTitleMetricExtractor\n",
      "Run TitleLengthMetricExtractor\n",
      "Run NumberOfParagraphMetricExtractor\n",
      "Run NumberOfTokenMetricExtractor\n",
      "Run BertScoreMetricExtractor\n",
      "Run RougeScoreMetricExtractor\n",
      "Run CorrectnessMetricExtractor\n",
      "Run CompletenessMetricExtractor\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Extract a bunch of metrics for evaluating those reports generated by different LLM models\n",
    "from collections import defaultdict\n",
    "from inspect import iscoroutinefunction\n",
    "\n",
    "from main.metrics import HasTitleMetricExtractor\n",
    "from main.metrics import TitleLengthMetricExtractor\n",
    "from main.metrics import NumberOfParagraphMetricExtractor\n",
    "from main.metrics import NumberOfTokenMetricExtractor\n",
    "from main.llm_as_judge import Reference\n",
    "from main.metrics import BertScoreMetricExtractor\n",
    "from main.metrics import RougeScoreMetricExtractor\n",
    "from main.metrics import CorrectnessMetricExtractor\n",
    "from main.metrics import CompletenessMetricExtractor\n",
    "\n",
    "\n",
    "model_used_for_evaluation = \"llama2:latest\"\n",
    "\n",
    "metric_extractors = [HasTitleMetricExtractor(),\n",
    "                     TitleLengthMetricExtractor(),\n",
    "                     NumberOfParagraphMetricExtractor(),\n",
    "                     NumberOfTokenMetricExtractor(),\n",
    "                     BertScoreMetricExtractor(reference=Reference(content=document.content)),\n",
    "                     RougeScoreMetricExtractor(reference=Reference(content=document.content)),\n",
    "                     CorrectnessMetricExtractor(client=llm_api_client, \n",
    "                                                model=model_used_for_evaluation, \n",
    "                                                reference=Reference(content=document.content)),\n",
    "                     CompletenessMetricExtractor(client=llm_api_client, \n",
    "                                                        model=model_used_for_evaluation, \n",
    "                                                        reference=Reference(content=document.content)),\n",
    "                    ]\n",
    "\n",
    "metrics = defaultdict(dict)\n",
    "for model, report in per_model_report.items():\n",
    "    print(f'evaluating model {model}')\n",
    "    for metric_extractor in metric_extractors:\n",
    "        print(f'Run {metric_extractor.__class__.__name__}')\n",
    "\n",
    "        if iscoroutinefunction(metric_extractor.extract):\n",
    "            metric = await metric_extractor.extract(report)\n",
    "        else:\n",
    "            metric = metric_extractor.extract(report)\n",
    "            \n",
    "        metrics[model][metric.name] = float(metric.value)\n",
    "    print(\"--------------------------------------------------------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6290b2cc-c39b-4fe8-add8-0107cff48a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  deepseek-r1:8b                qwen3-vl:8b                   llama2:latest                 \n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "Has-Title-Metric                                  1.0                           1.0                           1.0                           \n",
      "Number-Of-Chars-In-Title-Metric                   72.0                          883.0                         41.0                          \n",
      "Number-Of-Paragraphs-Metric                       5.0                           4.0                           4.0                           \n",
      "Number-Of-Tokens-Metric                           162.0                         125.0                         268.0                         \n",
      "Bert-Score-Metric                                 0.6435026526451111            0.6124502420425415            0.7325177192687988            \n",
      "Rouge-Score-Metric                                0.3652694610778443            0.421875                      0.7955390334572491            \n",
      "Correctness-Metric                                1.0                           1.0                           1.0                           \n",
      "Completeness-Metric                               1.0                           1.0                           1.0                           \n"
     ]
    }
   ],
   "source": [
    "# show metrics from different models\n",
    "cols = [f'{\"\":<50}']\n",
    "for model, _ in metrics.items():\n",
    "    cols.append(f'{model:<30}')\n",
    "print(''.join(cols))\n",
    "print(f'----------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "candidate_models = [\"deepseek-r1:8b\", \"qwen3-vl:8b\", \"llama2:latest\", ]\n",
    "for name,_ in metrics[\"deepseek-r1:8b\"].items():\n",
    "    cols = [f'{name:<50}']\n",
    "    for candidate_model in candidate_models:\n",
    "        value = metrics[candidate_model][name]\n",
    "        cols.append(f'{value:<30}')\n",
    "    print(''.join(cols))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
